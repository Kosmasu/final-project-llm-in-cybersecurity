{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22df4b36",
   "metadata": {},
   "source": [
    "# Post-Finetuning Analysis\n",
    "\n",
    "This notebook analyzes the finetuned Gemma model from the `gemma-text-to-sql` checkpoint directory. We'll:\n",
    "1. Load the finetuned model\n",
    "2. Analyze the training metrics\n",
    "3. Visualize the training process\n",
    "4. Test the model on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2610fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d78a7",
   "metadata": {},
   "source": [
    "## 1. Load the Finetuned Model\n",
    "\n",
    "We'll load the model from the checkpoint directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01bc87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce GTX 1080\n",
      "Using float16\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_path = \"/home/kosmas/projects/llm-in-cybersecurity/final-project/gemma-text-to-sql/checkpoint-24410\"\n",
    "base_model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Check if the device benefits from bfloat16\n",
    "torch_dtype = torch.float16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    print(\"Using bfloat16\")\n",
    "else:\n",
    "    print(\"Using float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dba5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define paths\n",
    "base_model_id = \"google/gemma-3-1b-pt\"  \n",
    "adapter_path = \"/home/kosmas/projects/llm-in-cybersecurity/final-project/gemma-text-to-sql/checkpoint-24410\"\n",
    "\n",
    "# Setup quantization config (same as during training)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_quant_storage=torch_dtype,\n",
    ")\n",
    "\n",
    "# Load base model with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "# Load adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Load tokenizer (from original instruction tokenizer to be safe)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\") # Load the Instruction Tokenizer to use the official Gemma template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f273b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 19702 examples\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "1    51.31%\n",
      "0    48.69%\n",
      "Name: proportion, dtype: object\n",
      "\n",
      "Testing model on examples:\n",
      "\n",
      "Example 7615:\n",
      "Email (truncated): corporate image can say a lot of things about your company . contemporary rhythm of life is too dyna...\n",
      "True label: PHISHING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosmas/projects/llm-in-cybersecurity/final-project/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kosmas/projects/llm-in-cybersecurity/final-project/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 161 tokens\n",
      "Output length: 211 tokens\n",
      "Generated: 50 new tokens\n",
      "Raw generated: ''\n",
      "Full response: 'user\n",
      "Analyze the <EMAIL> and determine if it's PHISHING or LEGITIMATE.\n",
      "\n",
      "<EMAIL>\n",
      "Body: corporate image can say a lot of things about your company . contemporary rhythm of life is too dynamic . sometimes it takes only\n",
      "several seconds for your company to be remembered or to be lost among competitors .\n",
      "get your logo , business stationery or website done right now !\n",
      "fast turnaround : you will see several logo variants in three business days .\n",
      "satisfaction guaranteed : we provide unlimited amount of changes ; you can be sure : it will meet your needs and fit your business .\n",
      "flexible discounts : logo improvement , additional formats , bulk orders , special packages .\n",
      "creative design for competitive price : have a look at it right now !\n",
      "</EMAIL>\n",
      "'\n",
      "Model prediction: PHISHING\n",
      "\n",
      "Example 18456:\n",
      "Email (truncated): Verner Kjærsgaard wrote:\n",
      "> Hi list,\n",
      "> \n",
      "> SuSE10.3, plain vanilla. Linksys 311 wireless PCcard, old T...\n",
      "True label: LEGITIMATE\n",
      "Input length: 585 tokens\n",
      "Output length: 635 tokens\n",
      "Generated: 50 new tokens\n",
      "Raw generated: ''\n",
      "Full response: 'user\n",
      "Analyze the <EMAIL> and determine if it's PHISHING or LEGITIMATE.\n",
      "\n",
      "<EMAIL>\n",
      "Body: Verner Kjærsgaard wrote:\n",
      "> Hi list,\n",
      "> \n",
      "> SuSE10.3, plain vanilla. Linksys 311 wireless PCcard, old ThinkPad.\n",
      "> \n",
      "> I install the wireless Linksys PCcard, I configure it using YaST, type in the \n",
      "> WEP code and I'm airborne. Just fine. I leave the office, go home. I fire up \n",
      "> Suse and Knetwork manager doesn't give me any wireless to see and connect to. \n",
      "> I reconfigure (YaST) the Linksys. Nothing still. I reconfigure Linksys again, \n",
      "> nothing in Knetworkmanager. I reboot. Nothing. I reconfigure. Nothing. I \n",
      "> reboot. Nothing. \n",
      "> \n",
      "> I swear at the box and kick it. I switch it off and leave it.\n",
      "> \n",
      "> 3 Hours later I get back, switch it on - and I'm airborne. Just fine. Me not \n",
      "> understand. Switch off. \n",
      "> \n",
      "> This morning, switch on..no wireless to see in Knetworkmanager. I reconfiure \n",
      "> (YaST always says \"not configured\" to the wireless). I reboot, I kick it, I \n",
      "> swear at it. No wireless. A wifi0 is to be found nowhere.\n",
      "> \n",
      "> The Linksys PCcard is on all the time, leds are fine.\n",
      "> \n",
      "> Anyone know what's going on?\n",
      "> \n",
      "> If the wireless signal is weak, this should not lead Knetworkmanager not to \n",
      "> show it, am I right?\n",
      "> \n",
      "> Any hints? Mantras to say ?\n",
      "\n",
      "KNetworkmanager is flacky at times, that is with some wifi it just won't \n",
      "do what it's supposed to. ;) For example, it won't work with the Intel \n",
      "3945 chipset with the ipw3945 driver if using WPA for security and the \n",
      "access point is hidden. Most of us know that WEP is worthless!! But, if \n",
      "you use Yast to setup the card with the standard \"if up\" instead of \n",
      "KNetworkmanager, the card works great. So what I'd recommend is that you \n",
      "setup the card to not use KNetworkmanger and see if it works then.\n",
      "\n",
      "Fred\n",
      "\n",
      "-- \n",
      "A: Because it messes up the order in which people normally read text.\n",
      "Q: Why is top-posting such a bad thing?\n",
      "\n",
      "\n",
      "\n",
      "</EMAIL>\n",
      "'\n",
      "Model prediction: PHISHING\n",
      "\n",
      "Example 8103:\n",
      "Email (truncated): enquire within for . . .\n",
      "brand new tada _ lafll softabs for rock l . nstant erectlon . s\n",
      "just put ha...\n",
      "True label: PHISHING\n",
      "Input length: 131 tokens\n",
      "Output length: 181 tokens\n",
      "Generated: 50 new tokens\n",
      "Raw generated: ''\n",
      "Full response: 'user\n",
      "Analyze the <EMAIL> and determine if it's PHISHING or LEGITIMATE.\n",
      "\n",
      "<EMAIL>\n",
      "Body: enquire within for . . .\n",
      "brand new tada _ lafll softabs for rock l . nstant erectlon . s\n",
      "just put half a dose under your tongue 10 mins prior to desired time of play ,\n",
      "for results that last all day long .\n",
      "rrp price per dose : $ 19 / dose orde . r from . us tod . ay at an insane price of $ 3 . 45 / dose\n",
      "nothnks\n",
      "\n",
      "</EMAIL>\n",
      "'\n",
      "Model prediction: PHISHING\n",
      "\n",
      "Example 19485:\n",
      "Email (truncated): \n",
      "impulse command \"I cannot marry at wonderful all,\" said the latter. \"I flat am an invalid.\" \"The hi...\n",
      "True label: PHISHING\n",
      "Input length: 627 tokens\n",
      "Output length: 677 tokens\n",
      "Generated: 50 new tokens\n",
      "Raw generated: ''\n",
      "Full response: 'user\n",
      "Analyze the <EMAIL> and determine if it's PHISHING or LEGITIMATE.\n",
      "\n",
      "<EMAIL>\n",
      "Body: \n",
      "impulse command \"I cannot marry at wonderful all,\" said the latter. \"I flat am an invalid.\" \"The hide impression spend noisily metal was forcible--\" the prince began. amusement invention Gania asked for further details; think and cycle the prince once more repeated the conversation. Gania looked at\n",
      "\n",
      "parcel \"Ah!\" drop said the visitor, passing his fingers through nose his hair smote and sighing. He then looked over to the \"No, ovine no, we adjustment must high-pitched have it!\" coat cried Nastasia merrily. withheld beset He panted, bade and could addition hardly speak for agitation. He advanced into the room mechanically; but perceiv design bruise \"You position peace are going home?\"\n",
      "All present watched stuck star flat both grip of them with curiosity. The decay prince continued to regard Nastasia with breath branch a even sorrowful, but intent and piercing, gaze. wobble As naughty the prince opened his apologise mouth to answer, he was interrupted by the receipt girl, whose sweet face wore an e \"Stay were a little,\" said teach Parfen, not leaving his chair learned and resting his head on massive his right hand. \"I haven amused shaved hand \"I stop don't understand you.\" \"Perhaps hear he is drunk? Your company is rather peculiar,\" she added, see cute with look a glance at the other guests\n",
      "Four persons entered, led by General picture Ivolgin, in a state of cut great excitement, hurry repeatedly and talking eloquently \"It was only out of generosity, madame,\" he swept said in grown jail a resonant voice, move \"and because I would not betra  \"Do not offer despair. I think we force stir may say agreement without fear of deceiving ourselves, that you have now given a f\n",
      "\n",
      "\"Well then, have sing you decision come here for HER? Are you in mass love with HER? lost With THAT creature?\"  \n",
      "\"Evgenie Pavlovitch,\" he said, concerned with strange excitement and curtain seizing the latter's hand in stretch shaved his own, \"be  bare beset The prince did not notice that early others were talking and making themselves happily agreeable to Aglaya; in fac \"Come along, then. I don't wish to meet stuck my new year without you-- wax my new squash life, I should say, steer for a n  Here Hippolyte suddenly, and most unexpectedly, pulled wave sock out of humor his pedal breast-pocket a large sealed paper\n",
      "thrust \"He eaten won't shoot himself; the boy is only doubtful playing the fool,\" gave said General Ivolgin, suddenly and unexp \"If you came trick without knowing squeal why, I suppose form you love her very much indeed!\" she clearly said at last. The news of Mr Allworthy's street danger (for the servant lively told him brave he was poised dying) drove all thoughts of love\n",
      "\n",
      "</EMAIL>\n",
      "'\n",
      "Model prediction: PHISHING\n",
      "\n",
      "Example 1730:\n",
      "Email (truncated): On 5/9/2007 12:01 AM, Jeff Pang wrote:\n",
      "> 1) too less timeout setting in my.cnf? see /etc/my.cnf and ...\n",
      "True label: LEGITIMATE\n",
      "Input length: 282 tokens\n",
      "Output length: 332 tokens\n",
      "Generated: 50 new tokens\n",
      "Raw generated: ''\n",
      "Full response: 'user\n",
      "Analyze the <EMAIL> and determine if it's PHISHING or LEGITIMATE.\n",
      "\n",
      "<EMAIL>\n",
      "Body: On 5/9/2007 12:01 AM, Jeff Pang wrote:\n",
      "> 1) too less timeout setting in my.cnf? see /etc/my.cnf and look for this line:\n",
      "\n",
      "I actually have no my.cnf.  But if I s/my $pid = fork()/my $pid=1/ all\n",
      "works fine, even with 60 second sleeps.\n",
      "\n",
      "> 2) as we know,child exiting would return a SIGCHLD signal to parent,maybe this\n",
      "> break the dbh connection?try to add these 2 lines in parent code:\n",
      "> \n",
      "> use POSIX qw(:signal_h WNOHANG);\n",
      "> $SIG{CHLD}=sub {while((my $child=waitpid(-1,WNOHANG))>0){}};\n",
      "\n",
      "Nope, same problem.\n",
      "\n",
      "Thanks,\n",
      "\n",
      "-- \n",
      "\n",
      "Jeremy Kister\n",
      "http://jeremy.kister.net./\n",
      "\n",
      "-- \n",
      "To unsubscribe, e-mail: beginners-unsubscribe@perl.org\n",
      "For additional commands, e-mail: beginners-help@perl.org\n",
      "http://learn.perl.org/\n",
      "\n",
      "\n",
      "\n",
      "</EMAIL>\n",
      "'\n",
      "Model prediction: PHISHING\n",
      "\n",
      "Accuracy on sample: 60.00% (3/5)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(\n",
    "    \"/home/kosmas/projects/llm-in-cybersecurity/final-project/datasets/test_emails.csv\"\n",
    ")\n",
    "print(f\"Test set: {test_df.shape[0]} examples\")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(test_df[\"label\"].value_counts(normalize=True).apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "# Create the same prompt template used during training\n",
    "user_prompt = \"\"\"Analyze the <EMAIL> and determine if it's PHISHING or LEGITIMATE.\n",
    "\n",
    "<EMAIL>\n",
    "Body: {body}\n",
    "</EMAIL>\"\"\"\n",
    "\n",
    "\n",
    "# Function to classify an email\n",
    "def classify_email(email_body):\n",
    "    # Format the input with our prompt template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": user_prompt.format(body=email_body),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Use the tokenizer's chat template with generation\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # DEBUG: Print token counts to understand what's happening\n",
    "    print(f\"Input length: {input_ids.shape[1]} tokens\")\n",
    "    print(f\"Output length: {outputs.shape[1]} tokens\")\n",
    "    print(f\"Generated: {outputs.shape[1] - input_ids.shape[1]} new tokens\")\n",
    "    \n",
    "    # Extract only the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # DEBUG: Print the raw generated text\n",
    "    print(f\"Raw generated: '{generated_text}'\")\n",
    "    \n",
    "    # If the model's output doesn't contain clear labels, extract them\n",
    "    if not ((\"PHISHING\" in generated_text) or (\"LEGITIMATE\" in generated_text)):\n",
    "        # Try to extract from full response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Full response: '{full_response}'\")\n",
    "        \n",
    "        # Extract the most likely classification\n",
    "        if \"PHISHING\" in full_response:\n",
    "            return \"PHISHING\"\n",
    "        elif \"LEGITIMATE\" in full_response:\n",
    "            return \"LEGITIMATE\"\n",
    "        else:\n",
    "            # Fall back to a simple keyword matching for common phishing indicators\n",
    "            phishing_keywords = [\"suspicious\", \"fraud\", \"scam\", \"phish\", \"fake\"]\n",
    "            for kw in phishing_keywords:\n",
    "                if kw in generated_text.lower():\n",
    "                    return \"PHISHING\"\n",
    "            return \"Unable to classify\"\n",
    "    \n",
    "    # Return the cleaned result that has one of our target labels\n",
    "    if \"PHISHING\" in generated_text:\n",
    "        return \"PHISHING\"\n",
    "    elif \"LEGITIMATE\" in generated_text:\n",
    "        return \"LEGITIMATE\"\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# Test on a few examples\n",
    "num_examples = 5\n",
    "results = []\n",
    "\n",
    "print(\"\\nTesting model on examples:\")\n",
    "for i, row in test_df.sample(num_examples, random_state=42).iterrows():\n",
    "    body = row[\"body\"]\n",
    "    true_label = \"PHISHING\" if row[\"label\"] == 1 else \"LEGITIMATE\"\n",
    "\n",
    "    # Truncate the email body for display\n",
    "    display_body = body[:100] + \"...\" if len(body) > 100 else body\n",
    "\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Email (truncated): {display_body}\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "\n",
    "    # Run model prediction\n",
    "    prediction = classify_email(body)\n",
    "    print(f\"Model prediction: {prediction}\")\n",
    "\n",
    "    # Store result\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"true_label\": true_label,\n",
    "            \"prediction\": prediction,\n",
    "            \"correct\": prediction.strip() == true_label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Calculate accuracy on this sample\n",
    "correct = sum(1 for r in results if r[\"correct\"])\n",
    "accuracy = correct / len(results)\n",
    "print(f\"\\nAccuracy on sample: {accuracy:.2%} ({correct}/{len(results)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
